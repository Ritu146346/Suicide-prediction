{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Suicide Rate Prediction with Machine Learning:**","metadata":{"id":"8nSuTBKsJraZ"}},{"cell_type":"markdown","source":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Suicide is a serious public health problem. The World Health Organization (WHO) estimates that every year close to 800 000 people take their own life, which is one person every 40 seconds and there are many more people who attempt suicide. Suicide occurs throughout the lifespan and was the second leading cause of death among 15-29-year-olds globally in 2016.\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The objective of this notebook is to predict the suicide rates using Machine Learning algorithms and analyzing them to find correlated factors causing increase in suicide rates globally. \n\nThe steps demonstrated in this notebook are: \n1. Loading the data\n2. Familiarizing with data\n3. Visualizing the data\n4. Data Preprocessing & EDA\n5. Splitting the data\n6. Training the data\n7. Model Performance Comparision\n8. Statistical Tests\n9. Conclusion\n\n","metadata":{"id":"UsdhTvYGJrab"}},{"cell_type":"code","source":"#importing required libraries\n\nimport pandas as pd\nimport numpy as np\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn","metadata":{"id":"rQta1dx2Jrad","outputId":"2d8108d0-7c3e-4367-f5f1-d1e5916312e0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1. Loading Data:**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The dataset is borrowed from Kaggle, https://www.kaggle.com/russellyates88/suicide-rates-overview-1985-to-2016. This is a compiled dataset pulled from four other datasets linked by time and place from year 1985 to 2016. The source of those datasets is WHO, World Bank, UNDP and a dataset published in Kaggle. \n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The overview of this dataset is, it has 27820 samples with 12 features. Download the dataset from the link provided.\n","metadata":{"id":"bDmOaIAyJrak"}},{"cell_type":"code","source":"#Loading data into dataframe\n\ndata = pd.read_csv(\"../input/suicide-rates-overview-1985-to-2016/master.csv\")\ndata.head()","metadata":{"id":"vO-gPhOxJral","outputId":"321670cf-d649-4a9c-fe2c-af58f5227989","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2. Familiarizing with Data:**\n\nIn this step, few dataframe methods are used to look into the data and its features.","metadata":{"id":"a97t_Zq8Jras"}},{"cell_type":"code","source":"#Shape of dataframe\n\ndata.shape","metadata":{"id":"WB_voVuuJras","outputId":"4d3902ca-c530-4b3f-e1ac-e4831745bdce","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Listing the features of the dataset\n\ndata.columns","metadata":{"scrolled":true,"id":"6eHW6CqwJrax","outputId":"ab2d08cf-6d97-47b5-ebc6-63c83a976b07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Renaming the columns names for convinience\n\ndata.columns = ['country', 'year', 'gender', 'age_group', 'suicide_count', 'population', 'suicide_rate', 'country-year', 'HDI for year',\n                'gdp_for_year', 'gdp_per_capita', 'generation']\ndata.columns","metadata":{"id":"Tcu07Vw-s_Ur","outputId":"368a53b9-2215-48f9-ba7b-a3e4e1b9e0af","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Renaming the columns names for convinience\n\ndata.columns = ['country', 'year', 'gender', 'age_group', 'suicide_count', \n                'population', 'suicide_rate', 'country-year', 'HDI for year',\n                'gdp_for_year', 'gdp_per_capita', 'generation']\ndata.columns","metadata":{"id":"J9NJm_cMJra1","outputId":"edc23f89-038e-47e0-f2df-bd1b0f10dd13","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Information about the dataset\n\ndata.info()","metadata":{"id":"UxqRUpRBJra5","outputId":"41514286-88ef-4224-b5d2-e27526ded050","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.age_group.value_counts()","metadata":{"id":"5pVv0ydpJra-","outputId":"e395f777-c883-4cc5-cf65-1306ebf63220","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.generation.value_counts()","metadata":{"id":"Pm4TWFvMJrbC","outputId":"aab2682c-dcfa-4b2f-d6b2-4247f52de843","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Observations:**\n* `HDI for year` column has missing values. None of the other columns have any missing values. So considering to remove HDI from the dataset. \n* The age feature has 6 unique age groups\n* `Age` is grouped into year buckets as categorical format which needs to be encoded.\n* `Gender` should be encoded.\n* Scale required numerical features.\n* The generation feature has 6 types of generations.\n* `Generation` could be encoded as well.","metadata":{"id":"XZu-bxGsJrbF"}},{"cell_type":"markdown","source":"#### Let's see how many countries are avaialble in the dataset:","metadata":{"id":"GMZie84BJrbG"}},{"cell_type":"code","source":"#Listing countries\n\ncountry = data.country.unique()\nprint(\"Number of countries:\", len(country))\ncountry","metadata":{"id":"oyXsHMrzJrbH","outputId":"2d1ec73e-aabb-455b-81ec-e607b2e7728f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3. Visualizing the data:**\n\nFew plots and graphs are displayed to find how the data is distributed and the how features are related to each other.","metadata":{"id":"aTAJqTWeJrbN"}},{"cell_type":"code","source":"data.hist(bins = 50,figsize = (15,11))","metadata":{"id":"oPy-YPrSJrbO","outputId":"59c7bebb-dfe7-462b-a5b5-ebac61591d8a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Correlation heatmap\n\nplt.figure(figsize=(7,5))\nsns.heatmap(data.corr(), annot=True, cmap='Oranges')\nplt.show()","metadata":{"id":"NZfAZZlJJrbS","outputId":"74afa27d-0c78-4d3f-dc9c-e2f6e5983ca3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Gender and suicide count bar plot\n\nplt.figure(figsize=(10,3))\nsns.barplot(data.suicide_count,data.gender)\nplt.title('Gender - Suicide Count Bar Plot')\nplt.show()","metadata":{"id":"wNUn9cLsJrbX","outputId":"b443ad2b-f001-4420-b667-1fe02f801f7e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The above bar plot shows that the suicide cases are more in male population.**\n\nBut lets see if this pattern exists in all the age groups and also generations. ","metadata":{"id":"tYP1xC2gJrbc"}},{"cell_type":"code","source":"#Age Group - Count Bar Plot Grouped by Gender\n\nplt.figure(figsize=(10,3))\nsns.barplot(x = \"age_group\", y = \"suicide_count\", hue = \"gender\", data = data)\nplt.title(\"Age Group - Count Bar Plot Grouped by Gender\")\nplt.show()","metadata":{"id":"HafmYUfCJrbd","outputId":"389c4ad5-1a5f-43f6-9ebc-0a939cc5dace","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generation - Count Bar Plot grouped by Gender\n\nplt.figure(figsize=(9,5))\nsns.barplot(x = \"generation\", y = \"suicide_count\", hue = \"gender\", data = data)\nplt.title('Generation - Count Bar Plot grouped by Gender')\nplt.show()","metadata":{"id":"s_0F0yBIJrbi","outputId":"7b703490-6af6-4472-edc4-accbb057eb0b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**From the above two bar plots, it is clear that men commit suicide considerably more than women irrespective of age group and generation they belong to.**\n\nNow, lets check the suicide cases based on the age group.","metadata":{"id":"TnfTjoP7Jrbn"}},{"cell_type":"code","source":"# Age Group and Suicide count bar plot\n\nplt.figure(figsize=(9,5))\nsns.barplot(x=data['age_group'], y=data['suicide_count'])\nplt.xlabel('Age Group')\nplt.ylabel('Suicide Count')\nplt.title('Age Group - Suicide Count Bar Plot')\nplt.show()","metadata":{"id":"tIz59nXA2bpP","outputId":"b32acc51-8108-4c8b-d9e4-05332f2359b7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above boxplot shows that the suicide cases are more in the age group of 35-54 years followed by 55- 74 years. The surprising part is that the suicide cases in 5-14 year age group even though they are very less, mostly in tens.","metadata":{"id":"TL_2puZK3vYc"}},{"cell_type":"code","source":"#Generation & Suicide Count Bar Plot\n\nplt.figure(figsize=(9,5))\nsns.barplot(x=data['generation'], y=data['suicide_count'])\nplt.xlabel('Generation')\nplt.ylabel('Suicide Count')\nplt.title('Generation - Suicide Count Bar Plot')\nplt.show()","metadata":{"scrolled":true,"id":"Yzfa-PlgJrbo","outputId":"b4d2d6d3-dca1-4801-8201-24f5b1f40c99","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The above boxplot shows that the suicide cases are more in the boomers, silent and X generations. These generations are made up of people born until 1976 based on the details provided.\n* On further observation, these generations are the ones were most of them are in the age group where most suicides occur.","metadata":{"id":"OHiuDaNLJrbt"}},{"cell_type":"code","source":"#Gender & Sucide Count grouped by Age Group bar plot\n\nplt.figure(figsize=(7,7))\nsns.barplot(y=\"gender\", x=\"suicide_count\", hue=\"age_group\", data=data)\nplt.title('Gender & Sucide Count grouped by Age Group')\nplt.show()","metadata":{"id":"u_7OxeeaJrbv","outputId":"3b783ab7-ed3f-48db-c9d5-deeeb458029d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n* From the above graph, we can infer that 35-54 years age group is more prone to suicides irrespective of the gender frollowed by 55-74 years age group. \n* All together, it is obvious that males tend to commit suicide more than female.","metadata":{"id":"RFh3cY8yJrb0"}},{"cell_type":"code","source":"#Gender & Sucide Count grouped by Generation bar plot\n\nplt.figure(figsize=(7,7))\nsns.barplot(y=\"gender\", x=\"suicide_count\", hue=\"generation\", data=data)\nplt.title('Gender & Sucide Count grouped by Generation')\nplt.show()","metadata":{"id":"yn05WNZpJrb2","outputId":"e19d544a-f117-4a78-b5f9-250a1dc46e59","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* In the case of generation, the Bloomers generation had more suicide cases followed by Silent generation irrespective of the gender.\n* Even when considered generation, males are more prone to commit suicide.","metadata":{"id":"BM1IXvwzJrb8"}},{"cell_type":"code","source":"#Country & Suicide_rate Bar plot\n\nplt.figure(figsize=(15,25))\nsns.barplot(x = \"suicide_rate\", y = \"country\", data = data)\nplt.title('Country - Suicide_rate Bar plot')\nplt.show()","metadata":{"id":"pwbi36P_Jrb9","outputId":"e61c3fe9-74ee-4778-e3b3-3bc6064533cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The above bar plot shows that the  highest suicide rate country is  Lithuania followed by Sri Lanka.**","metadata":{"id":"ExIyevw4JrcC"}},{"cell_type":"code","source":"#Line plpot of year and suicide_rate\n\ndata[['year','suicide_rate']].groupby(['year']).sum().plot()","metadata":{"id":"vuV5pK1WJrcD","outputId":"694334b9-eeef-4a4b-b3f8-25bdb8084661","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The observations from the above plot are that the suicide rate had grown rapidly from year 1990 & the rate of suicide has drastically reduced in year 2016.** The dataset was collected during early 2016. So all the suicide cases of 2016 are not recorded in the dataset.","metadata":{"id":"U3xFvdY5JrcH"}},{"cell_type":"code","source":"#Scatter matrix for checking outlier\n\nplt.figure(figsize=(20,10))\nattributes = ['suicide_count', 'population', 'suicide_rate','HDI for year', \n              'gdp_for_year','gdp_per_capita']\nscatter_matrix(data[attributes], figsize=(20,10))\nplt.show()","metadata":{"id":"qDRO2VXTBBFD","outputId":"71c38b36-2490-45aa-cc8d-b212188d23e0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4. Data Preprocessing & EDA:**\n\nHere, we clean the data by applying data preprocesssing techniques and transform the data to use it in the models.","metadata":{"id":"fG2WAo9eJrcI"}},{"cell_type":"code","source":"data.describe()","metadata":{"id":"-l05DfNnJrcJ","outputId":"6b1855fa-7cb6-49ae-dc75-3154e83e6c34","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the data for null or missing values\n\ndata.isnull().sum()","metadata":{"id":"gEKRQmQdJrcO","outputId":"87b2ae95-ce61-4965-bc65-d9f44bc5d016","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above stats, it is clear that the column, `HDI for year` has 19456 null values out of 27820 samples which is approximately 70% of the column. This may tamper the model performance so, dropping the `HDI for year` column from the dataset.","metadata":{"id":"z8uvkXaJJrcT"}},{"cell_type":"code","source":"#dropping the HDI for year column\n\ndata = data.drop(['HDI for year'], axis = 1)\ndata.shape","metadata":{"id":"PI146ZvIJrcU","outputId":"792706d5-1fc9-4f27-817f-16fd786f9ed7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"id":"a9MKNEgjJrca","outputId":"9e03d747-c72c-4749-8977-3f73486b7109","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The column country-year is just a combination of country and year columns. So dropping that column.","metadata":{"id":"xpVKRgsYJrce"}},{"cell_type":"code","source":"#dropping the country-year for year column\n\ndata = data.drop(['country-year'], axis = 1)\ndata.shape","metadata":{"id":"gSn8Pg4rJrcf","outputId":"635a2d8f-889d-4af4-a2b9-0f6cfc47c073","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going further with 10 features which also include the target column. For further assurance, lets drop all the null rows from the dataset.","metadata":{"id":"Sa6vo9J9Jrck"}},{"cell_type":"code","source":"#droppinf off any null rows (is any)\n\ndata = data.dropna()\ndata.shape","metadata":{"id":"xLLiugQDJrcl","outputId":"f38da4dd-7d51-42d6-aa72-ab6134fad0b7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The non-numerical labeled columns, country, year, gender, age_group and generation are to be converted to numerical labels that can be don by using SkLearn's LabelEncoder.","metadata":{"id":"aOwL6v_TJrc8"}},{"cell_type":"code","source":"#encoding the categorical features with LabelEncoder\n\nfrom sklearn.preprocessing import LabelEncoder\ncategorical = ['country', 'year','age_group', 'gender', 'generation']\nle = sklearn.preprocessing.LabelEncoder()\n\nfor column in categorical:\n    data[column] = le.fit_transform(data[column])","metadata":{"id":"7wWr9BSnhtDb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating a copy of dataset for statistical test\n\nstat_data = data.copy()\nstat_data","metadata":{"id":"2874A9Kjl6oc","outputId":"f381479a-287d-49bc-fc0a-8e0ea5de73f2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data. So, the numerical columns, `population`, `gdp_for_year` & `gdp_per_capita` are being standardized using SkLearn's RobustScalar.","metadata":{"id":"1syJ6q7OJrcp"}},{"cell_type":"code","source":"#Checking the data type of each column\n\ndata.dtypes","metadata":{"scrolled":true,"id":"u_CUS3-cJrcq","outputId":"d3fca057-e50c-413f-a803-9f85d51a465c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the column 'gdp_for_year' to float from object\n\ndata['gdp_for_year'] = data['gdp_for_year'].str.replace(',','').astype(float)","metadata":{"id":"Sw2_Re7xJrcz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scaling the numerical data columns with RobustScalar\n\nnumerical = ['suicide_count', 'population', 'suicide_rate', \n              'gdp_for_year','gdp_per_capita']\n\nfrom sklearn.preprocessing import RobustScaler\n\nrc = RobustScaler()\ndata[numerical] = rc.fit_transform(data[numerical])","metadata":{"id":"uEeVV8tYb6Wh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"id":"KCnkaPqLJrc-","outputId":"7b276ad8-cdeb-481c-ae42-a5ec6bbe096e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **5. Splitting the Data:**\n\nThe data is split into train & test sets, 80-20 split.","metadata":{"id":"WGrtnDb1JrdC"}},{"cell_type":"code","source":"# Sepratating & assigning features and target columns to X & y\n\ny = data['suicide_rate']\nX = data.drop('suicide_rate',axis=1)\nX.shape, y.shape","metadata":{"id":"8eDXiI-bJrdD","outputId":"36f4865b-8b88-421f-9d0a-08333117c5aa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the dataset into train and test sets: 80-20 split\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 12)\nX_train.shape, X_test.shape","metadata":{"scrolled":true,"id":"rvt2xPcFJrdI","outputId":"3be01a53-faca-4af2-fa91-198eabe61fe4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **6. Model Building & Training:**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Supervised machine learning is one of the most commonly used and successful types of machine learning. Supervised learning is used whenever we want to predict a certain outcome/label from a given set of features, and we have examples of features-label pairs. We build a machine learning model from these features-label pairs, which comprise our training set. Our goal is to make accurate predictions for new, never-before-seen data.\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There are two major types of supervised machine learning problems, called classification and regression. Our data set comes under regression problem, as the prediction of suicide rate is a continuous number, or a floating-point number in programming terms. The supervised machine learning models (regression) considered to train the dataset in this notebook are:\n* k-Nearest Neighbors Regression \n* Linear Regression\n* Decision Tree\n* Random Forest\n* Gradient Boosting\n* Multilayer Perceptrons\n* XGBoost\n* Bagging Regression\n* Custom Ensemble: SuperLearner\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The metrics considered to evaluate the model performance are Accuracy & Root Mean Squared Error.\n\n","metadata":{"id":"xvU-3zOJJrdO"}},{"cell_type":"code","source":"#importing required libraries \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV","metadata":{"id":"4-FiFFE8U3Np","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating holders to store the model performance results\nML_Model = []\nacc_train = []\nacc_test = []\nrmse_train = []\nrmse_test = []\n\n#function to call for storing the results\ndef storeResults(model, a,b,c,d):\n    ML_Model.append(model)\n    acc_train.append(round(a, 3))\n    acc_test.append(round(b, 3))\n    rmse_train.append(round(c, 3))\n    rmse_test.append(round(d, 3))","metadata":{"id":"XnnP3JBdsV3_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **6.1. k-Nearest Neighbors Regression:**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K nearest neighbors is a simple algorithm that stores all available cases and predict the numerical target based on a similarity measure (e.g., distance functions). A simple implementation of KNN regression is to calculate the average of the numerical target of the k nearest neighbors.","metadata":{"id":"UJzPG709Mg1q"}},{"cell_type":"code","source":"# KNN Regression model\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# instantiate the model\nknn = KNeighborsRegressor()\n\nparam_grid = {'n_neighbors':list(range(1, 31)), 'weights': ['uniform', 'distance']}\n\n# instantiate the grid\nknn_grid = GridSearchCV(knn, param_grid , cv=10)\n\n# fit the model \nknn_grid.fit(X_train, y_train)","metadata":{"id":"YA8km8bBOBXN","outputId":"e3564bff-9c04-464f-c959-a15ac71cf827","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the best parameters for the model\nknn_para = knn_grid.best_params_\nprint(knn_para)","metadata":{"id":"3_mRw5R3q39p","outputId":"0777d814-6a1d-473f-fa2c-b658a76e64bf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicting the target value from the model for the samples\ny_train_knn = knn_grid.predict(X_train)\ny_test_knn = knn_grid.predict(X_test)","metadata":{"id":"HYwHEDsFTeEf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Performance Evaluation:**","metadata":{"id":"ezJTO40RTaEr"}},{"cell_type":"code","source":"#computing the accuracy of the model performance\nacc_train_knn = knn_grid.score(X_train, y_train) \nacc_test_knn = knn_grid.score(X_test, y_test)\n\n#computing root mean squared error (RMSE)\nrmse_train_knn = np.sqrt(mean_squared_error(y_train, y_train_knn))\nrmse_test_knn = np.sqrt(mean_squared_error(y_test, y_test_knn))\n\nprint(\"KNN: Accuracy on training Data: {:.3f}\".format(acc_train_knn))\nprint(\"KNN: Accuracy on test Data: {:.3f}\".format(acc_test_knn))\nprint('\\nKNN: The RMSE of the training set is:', rmse_train_knn)\nprint('KNN: The RMSE of the testing set is:', rmse_test_knn)","metadata":{"id":"Or3CUzLxTJeh","outputId":"62e8dd1c-85b9-4598-d602-3ea2796907d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Storing Results:**","metadata":{"id":"WPFWrzg_xfY6"}},{"cell_type":"code","source":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('k-Nearest Neighbors Regression', acc_train_knn, acc_test_knn, rmse_train_knn, rmse_test_knn)","metadata":{"id":"AgbKwIMFs0IL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluating training and testing set performance with different numbers of neighbors from 1 to 30. The plot shows the training and test set accuracy on the y-axis against the setting of\nn_neighbors on the x-axis.","metadata":{"id":"YhHLCh6fcBkH"}},{"cell_type":"code","source":"training_accuracy = []\ntest_accuracy = []\n# try n_neighbors from 1 to 20\nneighbors_settings = range(1, 31)\nfor n in neighbors_settings:\n    # fit the model\n    knn = KNeighborsRegressor(n_neighbors=n)\n    knn.fit(X_train, y_train)\n    # record training set accuracy\n    training_accuracy.append(knn.score(X_train, y_train))\n    # record generalization accuracy\n    test_accuracy.append(knn.score(X_test, y_test))\n\n#plotting the training & testing accuracy for n_neighbours from 1 to 30\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")  \nplt.xlabel(\"n_neighbors\")\nplt.legend()","metadata":{"id":"xtSHKTBkZqIP","outputId":"1e51e24e-3426-4073-f09a-bf7eca771978","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**OBSERVATIONS:** This discrepancy between performance on the training set and the testing set fro n_neighbors < 5 is a clear sign of overfitting. After that, the perfromance is not so great so, moving on to the other models.","metadata":{"id":"S9RDEIXMVNEt"}},{"cell_type":"markdown","source":"### **6.2. Linear Regression**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Linear regression, or ordinary least squares (OLS), is the simplest and most classic linear method for regression. Linear regression finds the parameters w and b that minimize the mean squared error between predictions and the true regression targets, y, on the training set.","metadata":{"id":"xU5ofQmeVNxT"}},{"cell_type":"code","source":"# Linear regression model \nfrom sklearn.linear_model import LinearRegression\n\n# instantiate the model\nlr = LinearRegression()\n# fit the model \nlr.fit(X_train, y_train)","metadata":{"id":"P3SXN6bPJrdP","outputId":"56321272-fd6f-464b-9735-796a6bca1e6e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicting the target value from the model for the samples\ny_test_lr = lr.predict(X_test)\ny_train_lr = lr.predict(X_train)","metadata":{"id":"Ft9JzfhlJrdV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Performance Evaluation:**","metadata":{"id":"h-_yC294Jrda"}},{"cell_type":"code","source":"#computing the accuracy of the model performance\nacc_train_lr = lr.score(X_train, y_train)\nacc_test_lr = lr.score(X_test, y_test)\n\n#computing root mean squared error (RMSE)\nrmse_train_lr = np.sqrt(mean_squared_error(y_train, y_train_lr))\nrmse_test_lr = np.sqrt(mean_squared_error(y_test, y_test_lr))\n\nprint(\"Linear Regression: Accuracy on training Data: {:.3f}\".format(acc_train_lr))\nprint(\"Linear Regression: Accuracy on test Data: {:.3f}\".format(acc_test_lr))\nprint('\\nLinear Regression: The RMSE of the training set is:', rmse_train_lr)\nprint('Linear Regression: The RMSE of the testing set is:', rmse_test_lr)","metadata":{"id":"bWbR7xx7Jrda","outputId":"365ae148-9d58-4cdb-d395-7fefb2b7f1e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Storing Results:**","metadata":{"id":"gVSA11JTxnmx"}},{"cell_type":"code","source":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('Linear Regression', acc_train_lr, acc_test_lr, rmse_train_lr, rmse_test_lr)","metadata":{"id":"7mDHSdiBvdZo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**OBSERVATIONS:** The model preformance is not very good, but we can see that the scores on the training and test sets are very close together. This means we are likely underfitting, not overfitting.","metadata":{"id":"zq4Dz3JPci6h"}},{"cell_type":"markdown","source":"***Note to Remember:*** The most common ML algorithms, logistic regression and linear support vector machines\n(linear SVMs), are supervised classification algorithms, can't be applied on regression problems.\nDespite its name, LogisticRegression is a classification algorithm and not a regression algorithm, and it should not be confused with LinearRegression.","metadata":{"id":"hA_3RDwErqTY"}},{"cell_type":"markdown","source":"### **6.3. Decision Trees:** *Regression*\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Decision trees are widely used models for classification and regression tasks. Essentially, they learn a hierarchy of if/else questions, leading to a decision. Learning a decision tree means learning the sequence of if/else questions that gets us to the true answer most quickly.\n \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the machine learning setting, these questions are called tests (not to be confused with the test set, which is the data we use to test to see how generalizable our model is). To build a tree, the algorithm searches over all possible tests and finds the one that is most informative about the target variable.\n","metadata":{"id":"-MQRjFKlJrdk"}},{"cell_type":"code","source":"# Decision Tree regression model \nfrom sklearn.tree import DecisionTreeRegressor\n\n# instantiate the model \ntree = DecisionTreeRegressor(max_depth=9)\n# fit the model \ntree.fit(X_train, y_train)","metadata":{"outputId":"8e594a98-b936-496a-b788-a25f9d975844","id":"Idsy3ifptXu3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicting the target value from the model for the samples\ny_test_tree = tree.predict(X_test)\ny_train_tree = tree.predict(X_train)","metadata":{"id":"R1tnPjILtXvL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Performance Evaluation:**","metadata":{"id":"RD-iEu_6tXvU"}},{"cell_type":"code","source":"#computing the accuracy of the model performance\nacc_train_tree = tree.score(X_train, y_train)\nacc_test_tree = tree.score(X_test, y_test)\n\n#computing root mean squared error (RMSE)\nrmse_train_tree = np.sqrt(mean_squared_error(y_train, y_train_tree))\nrmse_test_tree = np.sqrt(mean_squared_error(y_test, y_test_tree))\n\nprint(\"Decision Tree: Accuracy on training Data: {:.3f}\".format(acc_train_tree))\nprint(\"Decision Tree: Accuracy on test Data: {:.3f}\".format(acc_test_tree))\nprint('\\nDecision Tree: The RMSE of the training set is:', rmse_train_tree)\nprint('Decision Tree: The RMSE of the testing set is:', rmse_test_tree)","metadata":{"outputId":"1cf1298c-bdab-4790-fe25-3152433384d1","id":"ivncKP53tXvV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Storing Results:**","metadata":{"id":"TsOYSDm6xqEY"}},{"cell_type":"code","source":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('Decision Tree',acc_train_tree, acc_test_tree, rmse_train_tree, rmse_test_tree)","metadata":{"id":"6CqtNbqD3u2R","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the feature improtance in the model\nplt.figure(figsize=(9,7))\nn_features = X_train.shape[1]\nplt.barh(range(n_features), tree.feature_importances_, align='center')\nplt.yticks(np.arange(n_features), X_train.columns)\nplt.xlabel(\"Feature importance\")\nplt.ylabel(\"Feature\")\nplt.show()","metadata":{"id":"Lek9bHNfQt6p","outputId":"db36b969-5014-433a-e197-93364d10f7dc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluating training and testing set performance with different numbers of max_depth from 1 to 30. The plot shows the training and test set accuracy on the y-axis against the setting of max_depth on the x-axis.","metadata":{"id":"aaiRdEnLNs12"}},{"cell_type":"code","source":"training_accuracy = []\ntest_accuracy = []\n# try max_depth from 1 to 30\ndepth = range(1, 31)\nfor n in depth:\n    # fit the model\n    tree = DecisionTreeRegressor(max_depth=n)\n    tree.fit(X_train, y_train)\n    # record training set accuracy\n    training_accuracy.append(tree.score(X_train, y_train))\n    # record generalization accuracy\n    test_accuracy.append(tree.score(X_test, y_test))\n\n#plotting the training & testing accuracy for max_depth from 1 to 30\nplt.plot(depth, training_accuracy, label=\"training accuracy\")\nplt.plot(depth, test_accuracy, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")  \nplt.xlabel(\"max_depth\")\nplt.legend()","metadata":{"id":"1k1b0vgXPyYS","outputId":"4a907d07-715c-4e16-b4e6-cad24900d6fd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**OBSERVATIONS:** The model preformance is gradually increased on incresing the max_depth parameter. But after max_depth = 9, the model overfits. So the model is considered with max_depth = 9 which has an accuracy of 95.2%.","metadata":{"id":"q2Pf85a4tXvm"}},{"cell_type":"markdown","source":"### **6.4. Random Forest:** *Ensemble of Decision Trees*\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Random forests for regression and classification are currently among the most widely used machine learning methods.A random forest is essentially a collection of decision trees, where each tree is slightly different from the others. The idea behind random forests is that each tree might do a relatively good job of predicting, but will likely overfit on part of the data.  \n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If we build many trees, all of which work well and overfit in different ways, we can reduce the amount of overfitting by averaging their results. To build a random forest model, you need to decide on the number of trees to build (the n_estimators parameter of RandomForestRegressor or RandomForestClassifier). They are very powerful, often work well without heavy tuning of the parameters, and don’t require scaling of the data.","metadata":{"id":"2JkVQ5-J5C3x"}},{"cell_type":"code","source":"# Random Forest regression model\nfrom sklearn.ensemble import RandomForestRegressor\n\n# instantiate the model\nforest = RandomForestRegressor(max_depth=9)\n\n# fit the model \nforest.fit(X_train, y_train)","metadata":{"id":"xUp8ngaKJrdn","outputId":"4bff8bca-ef8f-497b-ec53-87b4b7202990","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicting the target value from the model for the samples\ny_test_forest = forest.predict(X_test)\ny_train_forest = forest.predict(X_train)","metadata":{"id":"HG2hMUFSJrdq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Performance Evaluation:**","metadata":{"id":"xYFpg7Z2wWU0"}},{"cell_type":"code","source":"#computing the accuracy of the model performance\nacc_train_forest = forest.score(X_train, y_train)\nacc_test_forest = forest.score(X_test, y_test)\n\n#computing root mean squared error (RMSE)\nrmse_train_forest = np.sqrt(mean_squared_error(y_train, y_train_forest))\nrmse_test_forest = np.sqrt(mean_squared_error(y_test, y_test_forest))\n\nprint(\"Random Forest: Accuracy on training Data: {:.3f}\".format(acc_train_forest))\nprint(\"Random Forest: Accuracy on test Data: {:.3f}\".format(acc_test_forest))\nprint('\\nRandom Forest: The RMSE of the training set is: ', rmse_train_forest)\nprint('Random Forest: The RMSE of the testing set is: ', rmse_test_forest)","metadata":{"id":"6fkhNFmKwWU3","outputId":"f5e4a93e-6367-4cdf-d843-097bfbd39a88","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Storing Results:**","metadata":{"id":"95yjoQaExuCr"}},{"cell_type":"code","source":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('Random Forest',acc_train_forest, acc_test_forest, rmse_train_forest, rmse_test_forest)","metadata":{"id":"lvLg6p-swWOG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluating training and testing set performance with different numbers of max_depth from 1 to 30. The plot shows the training and test set accuracy on the y-axis against the setting of max_depth on the x-axis.","metadata":{"id":"aMr2m2NCN_L_"}},{"cell_type":"code","source":"training_accuracy = []\ntest_accuracy = []\n# try max_depth from 1 to 30\ndepth = range(1, 31)\nfor n in depth:\n    # fit the model\n    forest = RandomForestRegressor(max_depth=n)\n    forest.fit(X_train, y_train)\n    # record training set accuracy\n    training_accuracy.append(forest.score(X_train, y_train))\n    # record generalization accuracy\n    test_accuracy.append(forest.score(X_test, y_test))\n\n#plotting the training & testing accuracy for max_depth from 1 to 30\nplt.plot(depth, training_accuracy, label=\"training accuracy\")\nplt.plot(depth, test_accuracy, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")  \nplt.xlabel(\"max_depth\")\nplt.legend()","metadata":{"id":"DTk19lnNfs9w","outputId":"165f7ecd-6057-4bfd-8850-221a070901dd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**OBSERVATIONS:** The random forest gives us an accuracy of 99.4%, better than the linear models or a single decision tree, without tuning any parameters. But this might also be a case of overfitting. So, the prarameter are tuned and the finalized model has an accuracy of 98% which is better than the linear & decision tree models.","metadata":{"id":"w7cBRwEKzFeP"}},{"cell_type":"markdown","source":"### **6.5. Multilayer Perceptrons (MLPs):** *Deep Learning*\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Multilayer perceptrons (MLPs) are\nalso known as (vanilla) feed-forward neural networks, or sometimes just neural networks. Multilayer perceptrons can be applied for both classification and regression problems. \n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MLPs can be viewed as generalizations of linear models that perform multiple stages of processing to come to a decision.","metadata":{"id":"DC-KHuGHPvXd"}},{"cell_type":"code","source":"# Multilayer Perceptrons model\nfrom sklearn.neural_network import MLPRegressor\n\n# instantiate the model\nmlp = MLPRegressor(hidden_layer_sizes=([100,100]))\n\n# fit the model \nmlp.fit(X_train, y_train)","metadata":{"id":"6neV_chFPvXg","outputId":"ce9fe1b5-88c3-481f-836a-62d08bcbf1ab","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicting the target value from the model for the samples\ny_test_mlp = mlp.predict(X_test)\ny_train_mlp = mlp.predict(X_train)","metadata":{"id":"lIh16U-DPvXw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Performance Evaluation:**","metadata":{"id":"rwDQRNsMPvX7"}},{"cell_type":"code","source":"#computing the accuracy of the model performance\nacc_train_mlp = mlp.score(X_train, y_train)\nacc_test_mlp = mlp.score(X_test, y_test)\n\n#computing root mean squared error (RMSE)\nrmse_train_mlp = np.sqrt(mean_squared_error(y_train, y_train_mlp))\nrmse_test_mlp = np.sqrt(mean_squared_error(y_test, y_test_mlp))\n\nprint(\"Multilayer Perceptron Regression: Accuracy on training Data: {:.3f}\".format(acc_train_mlp))\nprint(\"Multilayer Perceptron Regression: Accuracy on test Data: {:.3f}\".format(acc_test_mlp))\nprint('\\nMultilayer Perceptron Regression: The RMSE of the training set is: ', rmse_train_mlp)\nprint('Multilayer Perceptron Regression: The RMSE of the testing set is: ', rmse_test_mlp)","metadata":{"id":"Gp33bgDAPvX8","outputId":"84750f91-bfda-45c3-a27c-97715c619543","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Storing Results:**","metadata":{"id":"AW_f1GSnSgQx"}},{"cell_type":"code","source":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('Multilayer Perceptron Regression',acc_train_mlp, acc_test_mlp, rmse_train_mlp, rmse_test_mlp)","metadata":{"id":"x-kGk0AeSgQ4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**OBSERVATIONS:** The model didnt overfit when trained without tuning any parameters. But, the model accuracy obtained is 89.2%. \n\nSo, hyperparameter tuning is performed for the model. The tuned parameters are number of hidden layers and the hidden_units of each layer with default values of alpha. The otimized Gradient Boosted model gives us an accuracy of 92.8%, with parameter tuning.","metadata":{"id":"kFcdLqStPvYR"}},{"cell_type":"markdown","source":"### **6.6. XGBoost Regression:**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;XGBoost is one of the most popular machine learning algorithms these days. XGBoost stands for eXtreme Gradient Boosting. Regardless of the type of prediction task at hand; regression or classification. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.","metadata":{"id":"OwtQg-D0PoSa"}},{"cell_type":"code","source":"#XGBoost Regression model\nfrom xgboost import XGBRegressor\n\n# instantiate the model\nxgb = XGBRegressor(learning_rate=0.2,max_depth=4)\n#fit the model\nxgb.fit(X_train, y_train)","metadata":{"id":"a9oA4gSOy0pQ","outputId":"1447fd32-8efc-4cad-cc31-3056eea9aadc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicting the target value from the model for the samples\ny_test_xgb = xgb.predict(X_test)\ny_train_xgb = xgb.predict(X_train)","metadata":{"id":"Fs_oMGV8XEG0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Performance Evaluation:**","metadata":{"id":"Ge4aLXoe1Vkg"}},{"cell_type":"code","source":"#computing the accuracy of the model performance\nacc_train_xgb = xgb.score(X_train, y_train)\nacc_test_xgb = xgb.score(X_test, y_test)\n\n#computing root mean squared error (RMSE)\nrmse_train_xgb = np.sqrt(mean_squared_error(y_train, y_train_xgb))\nrmse_test_xgb = np.sqrt(mean_squared_error(y_test, y_test_xgb))\n\nprint(\"XGBoost Regression: Accuracy on training Data: {:.3f}\".format(acc_train_xgb))\nprint(\"XGBoost Regression: Accuracy on test Data: {:.3f}\".format(acc_test_xgb))\nprint('\\nXGBoost Regression: The RMSE of the training set is: ', rmse_train_xgb)\nprint('XGBoost Regression: The RMSE of the testing set is: ', rmse_test_xgb)","metadata":{"id":"BsJE7kgXy0lV","outputId":"698eca21-edd6-4039-f359-b1a0cae7d630","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Storing Results:**","metadata":{"id":"5-4kaNx_Uk-3"}},{"cell_type":"code","source":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('XGBoost Regression',acc_train_xgb, acc_test_xgb, rmse_train_xgb, rmse_test_xgb)","metadata":{"id":"1kKbUf7rUk-4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**OBSERVATIONS:** Upon tuning the hyperparameter, the model performance increased, and the resulting model performance is 98.8%.","metadata":{"id":"Z2iN8S91ng02"}},{"cell_type":"markdown","source":"### **6.7. Custom Ensemble - SuperLearner:**\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To build a custom ensemble, a Python library called mlens is used. mlens is short of ML-Ensemble used for memory efficient parallelized ensemble learning. ML-Ensemble is a library for building Scikit-learn compatible ensemble estimator. Ensembles are built as a feed-forward network, with a set of layers stacked on each other. ","metadata":{"id":"smZK5RdnJrdy"}},{"cell_type":"code","source":"### You will need mlens package\n!pip install mlens","metadata":{"id":"pBZDqk4bzcPO","outputId":"d67a61ea-2968-49e1-89e6-a103b2503bed","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mlens.ensemble import SuperLearner\nfrom mlens.model_selection import Evaluator\nfrom mlens.metrics import make_scorer\nfrom mlens.metrics.metrics import rmse\n\nfrom sklearn.metrics import accuracy_score","metadata":{"id":"qXvaoKnJJrdy","outputId":"2e4eb2fe-6c46-40fa-b262-16b75d74ee1f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Build ---\n# Passing a scoring function will create cv scores during fitting \n#the scorer should be a simple function accepting to vectors and returning a scalar\nensemble = SuperLearner(scorer=rmse, random_state=555, verbose=2)\n\n# Build the first layer\nensemble.add(mlp)\nensemble.add(knn_grid)\nensemble.add_meta(lr)","metadata":{"id":"4rO0S0HaJrd2","outputId":"f9562c00-9ad1-4a69-8152-61f34964807c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit ensemble\nensemble.fit(X_train, y_train)","metadata":{"id":"wAmnQsaeJrd5","outputId":"2bf506a8-5b94-404a-db6f-891b36e67314","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicting the target of samples from the model\ny_train_en = ensemble.predict(X_train)\ny_test_en = ensemble.predict(X_test)","metadata":{"id":"amIIPBijz0uQ","outputId":"6a0cf010-9884-401f-c25b-a2555abbea4d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Performance Evaluation:**","metadata":{"id":"5hYOzfmF0L_b"}},{"cell_type":"code","source":"#computing the accuracy of the model performance\nacc_train_en = sklearn.metrics.r2_score(y_train,y_train_en)\nacc_test_en = sklearn.metrics.r2_score(y_test,y_test_en)\n\n#computing root mean squared error (RMSE)\nrmse_train_en = rmse(y_train,y_train_en)\nrmse_test_en = rmse(y_test,y_test_en)\n\nprint(\"Custom Ensemble: Accuracy on training Data: {:.3f}\".format(acc_train_en))\nprint(\"Custom Ensemble: Accuracy on test Data: {:.3f}\".format(acc_test_en))\nprint('\\nCustom Ensemble: The RMSE of the training set is: ', rmse_train_en)\nprint('Custom Ensemble: The RMSE of the testing set is: ', rmse_test_en)","metadata":{"id":"B1ckXk6j0L_e","outputId":"36c711fb-70c0-4a85-bef8-d46106682a89","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Storing Results:**","metadata":{"id":"xCMRaA_6QVLM"}},{"cell_type":"code","source":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('Ensemble_SuperLearner',acc_train_en, acc_test_en, rmse_train_en, rmse_test_en)","metadata":{"id":"p1f7h_vaQVLT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**OBSERVATIONS:**  The above results show that the ensemble model performance is much better than linear regression model. In a way this model is an improved model of the models used in this.","metadata":{"id":"JJv_m94S7DNY"}},{"cell_type":"markdown","source":"## **7. Comparision of Models:**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To compare the models performance, a dataframe is created. The columns of this dataframe are the lists created to store the results of the model.","metadata":{"id":"LRWt_1lqru5D"}},{"cell_type":"code","source":"#creating dataframe\nresults = pd.DataFrame({ 'ML Model': ML_Model,    \n    'Train Accuracy': acc_train,\n    'Test Accuracy': acc_test,\n    'Train RMSE': rmse_train,\n    'Test RMSE': rmse_test})","metadata":{"id":"8ULP9fmGVZBQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"id":"5maRTZ_gqm1y","outputId":"0da0e6f8-ee3b-4df3-c894-d1375f079e49","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sorting the datafram on accuracy\nresults.sort_values(by=['Test Accuracy', 'Train Accuracy'], ascending=False)","metadata":{"id":"LY5ZszLkXzbV","outputId":"e6069412-cfd1-4923-8c4a-8732d08fc272","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**OBSERVATIONS:** Among all the trained modesl, XGBoost performance is better. It is understandable because this model is very good in execution Speed & model performance.","metadata":{"id":"YKiJahz0Ur1J"}},{"cell_type":"markdown","source":"## **8. Statistical Tests:**\n\nStatistical tests are used in hypothesis testing. They can be used to:\n* determine whether a predictor variable has a statistically significant relationship with an outcome variable.\n* estimate the difference between two or more groups\n","metadata":{"id":"ueFbYumEUSgW"}},{"cell_type":"code","source":"#improting required libraries\nfrom scipy import stats","metadata":{"id":"5ykkcV1mlVA4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **8.1. Test 1: To check the difference in suicide rates between male and female**\nUsing independent sample t-test to check the difference in suicide rates between male and female. The hypothesis statements for this test are: \n\n**H0:** There is no difference in the suicide rates among male and female (Null).<br>\n**H1:** There is difference in the suicide rates among male and female (Alternate).\n","metadata":{"id":"suzEOXz9fvsC"}},{"cell_type":"code","source":"#collecting male suicide rate data\nmale = stat_data['suicide_rate'][stat_data['gender'] == 1]\nmale","metadata":{"id":"hp7YwoGLdvjL","outputId":"e9b5aae9-77ba-466b-f6e4-1a5ed7d73eb6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#collecting female suicide rate data\nfemale = stat_data['suicide_rate'][stat_data['gender'] == 0]\nfemale","metadata":{"id":"AXVRAePOef_Y","outputId":"84f4f5e8-0a30-4698-e244-a6802b594294","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculating p value\nttest,pval = stats.ttest_rel(male, female)\n\nif pval<0.05:\n    print(\"Reject null hypothesis\")\nelse:\n    print(\"Accept null hypothesis\")","metadata":{"id":"RbvJ1LdWfWZ8","outputId":"a0f21feb-db49-4e7a-c9c1-edc35be201cb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test Conclusion:** By performing T-test, the result obtained is to reject the null hypothesis. This basically means that there is different in suicide rates of male & female.","metadata":{"id":"xdjfBLjYTPU2"}},{"cell_type":"markdown","source":"### **8.2. Test 2: To find out the dependence of suicide rate on the age.**\nFinding out whether there is a dependence of suicide rate on the age using the Chi- Square test. The hypothesis statements for this test are: \n\n**H0:** Suicide rate and age are independent (Null).<br>\n**H1:** Suicide rate and age are dependent (Alternate). ","metadata":{"id":"K6XVke9WgQQ7"}},{"cell_type":"code","source":"#Creating Contingency Table\ncontingency_table = pd.crosstab(stat_data.suicide_rate, stat_data.age_group)","metadata":{"id":"0xO2wbYn9a0L","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Significance Level 5%\nalpha=0.05","metadata":{"id":"dVqqHqtG_Krs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chistat, p, dof, expected = stats.chi2_contingency(contingency_table )","metadata":{"id":"kymhWVst9rIU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#critical_value\ncritical_value=stats.chi2.ppf(q=1-alpha,df=dof)\nprint('critical_value:',critical_value)","metadata":{"id":"4Q5yJrJB_vUk","outputId":"959105ca-aff9-4614-fc44-83dbfce3acf2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Significance level: ',alpha)\nprint('Degree of Freedom: ',dof)\nprint('chi-square statistic:',chistat)\nprint('critical_value:',critical_value)\nprint('p-value:',p) \n#Here, pvalue = 0.0 and a low pvalue suggests that your sample provides enough evidence that you can reject  H0  for the entire population.","metadata":{"id":"JyXUEbDe_ghn","outputId":"7812b41e-8fe1-40de-803f-1e3cff9c5ecf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #compare chi_square_statistic with critical_value and p-value which is the \n #probability of getting chi-square>0.09 (chi_square_statistic)\nif chistat>=critical_value:\n    print(\"Reject H0,There is a dependency between Age group & Suicide rate.\")\nelse:\n    print(\"Retain H0,There is no relationship between Age group & Suicide rate.\")\n    \nif p<=alpha:\n    print(\"Reject H0,There is a dependency between Age group & Suicide rate.\")\nelse:\n    print(\"Retain H0,There is no relationship between Age group & Suicide rate.\")","metadata":{"id":"YF-fkzeTAKCU","outputId":"98f20ec3-93fa-4737-fb03-758ba230f3ac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test Conclusion:** By performing Chi- Square test, the result obtained is to reject the null hypothesis. This basically means that there is dependency between Age group & Suicide rate.","metadata":{"id":"_hYKiQ2sToc7"}},{"cell_type":"markdown","source":"## **9. Conclusion:**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The final take away form this project is the working of different machine learning models on a dataset and understanding their parameters. Creating this notebook helped me to learn a lot about the parameters of the models, how to tuned them and how they affect the model performance. \nThe final conclusion on the suicide dataset are that the irrespective of age group and generation, male population are more prone to commit suicide than female. ","metadata":{"id":"mzg0YuqTLJoc"}}]}